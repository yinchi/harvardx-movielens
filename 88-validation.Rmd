# Final validation

We now choose our best model \@ref(eq:final-model) for validation with the `validation` dataset.


The final RMSE computed with the `validation` set is:

```{r Final-RMSE-validation-set}
predicted_ratings_FINAL_VALIDATION <- validation |>
  mutate(weekNum = (timestamp - min(timestamp)) |>
           as.numeric(unit = "days") |> {\(x) floor(x/7) + 1}() ) |> 
  mutate(f_t = f_t[weekNum]) |> 
  left_join(movie_biases_reg, by='movieId') |>
  left_join(user_biases_reg, by='userId') |>
  left_join(genre_biases_reg, by='genres') |>
  mutate(pred = mu + b_i + b_u + b_g + f_t +
           map2_dbl(userId, movieId, \(u,v) U[Uidx[u],] %*% V[Vidx[v],])) |>
  pull(pred) |> clamp()

# Compute RMSE and add to data.table
RMSE(predicted_ratings_FINAL_VALIDATION, validation$rating)
```

The plot below plots a histogram of prediction errors:

```{r Prediction-error-plot, fig.height=3, fig.width=4}
par(cex = 0.7)
hist(predicted_ratings_FINAL_VALIDATION - validation$rating, 50,
     xlab = 'Prediction Error', main = '')
```


Here is a plot of the cumulative distribution of the absolute error:
```{r Absolute-error-ECDF, fig.height=4, fig.width=4}
the_ecdf <- ecdf(predicted_ratings_FINAL_VALIDATION - validation$rating)
par(cex = 0.7)
plot(seq(0,4.5,0.001), the_ecdf(seq(0,4.5,0.001)), 'l',
     xlab = 'Absoulute error of prediction', ylab = 'Empirical CDF')
```

What proportion of predictions are within half a star of the actual rating?

```{r Within-half-star}
mean(abs(predicted_ratings_FINAL_VALIDATION - validation$rating) < 0.5)
```

How many parameters are there in the model?

```{r Num-parameters}
nrow(movie_biases_reg) + nrow(user_biases_reg) + nrow(genre_biases_reg) +
  length(f_t) + length(U) + length(V)
```

Save the model:

```{r Save-model}
save(mu, movie_biases_reg, user_biases_reg, genre_biases_reg,
     f_t, Uidx, Vidx, U, V, file = 'FINAL_model.Rdata')
```

